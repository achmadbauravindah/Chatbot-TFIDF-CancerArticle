{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot-TFIDF-CancerArticle.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4niqSiT8IrGVjjR7tmp2f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achmadbauravindah/Chatbot-TFIDF-CancerArticle/blob/main/ChatBot_TFIDF_CancerArticle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtLc1J9RYYMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1c319c-fbc3-4b6e-ed40-fbc92c58e689"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random\n",
        "import requests\n",
        "nltk.download('popular', quiet=True) # for downloading packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Document\n",
        "URL = \"https://raw.githubusercontent.com/AchmadBauravindah/datas_for_ai/main/nlp/document_for_chatbot_answers.txt\"\n",
        "req = requests.get(URL)\n",
        "document = req.text"
      ],
      "metadata": {
        "id": "3nZAMPoJdozx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make document to lowercase words\n",
        "document = document.lower()"
      ],
      "metadata": {
        "id": "ZKD_6zgRRuY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sentences and word corpus\n",
        "sentence_tokens = nltk.sent_tokenize(document)\n",
        "word_tokens = nltk.word_tokenize(document)"
      ],
      "metadata": {
        "id": "KJIEuYZbBE3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Sentences Tokens\n",
        "sentence_tokens[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z2ud404R8gv",
        "outputId": "8885c801-b492-44f1-cd72-94c0b7b532c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this article is about the group of diseases.',\n",
              " 'for other uses, see cancer (disambiguation).',\n",
              " 'cancer\\nother names\\tmalignant tumor, malignant neoplasm\\ntumor mesothelioma2 legend.jpg\\na coronal ct scan showing a malignant mesothelioma\\nlegend: ‚Üí tumor ‚Üê, ‚ú± central pleural effusion, 1 & 3 lungs, 2 spine, 4 ribs, 5 aorta, 6 spleen, 7 & 8 kidneys, 9 liver\\npronunciation\\t\\n/Ààk√¶ns…ôr/ (listen)\\nspecialty\\toncology\\nsymptoms\\tlump, abnormal bleeding, prolonged cough, unexplained weight loss, change in bowel movements[1]\\nrisk factors\\texposure to carcinogens, tobacco, obesity, poor diet, lack of physical activity, excessive alcohol, certain infections[2][3]\\ntreatment\\tradiation therapy, surgery, chemotherapy, and targeted therapy.',\n",
              " '[2][4]\\nprognosis\\taverage five year survival 66% (usa)[5]\\nfrequency\\t24 million annually (2019)[6]\\ndeaths\\t10 million annually (2019)[6]\\ncancer is a group of diseases involving abnormal cell growth with the potential to invade or spread to other parts of the body.',\n",
              " '[2][7] these contrast with benign tumors, which do not spread.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Word Tokens \n",
        "word_tokens[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-wqvBAHSUE7",
        "outputId": "dfcec00b-4014-4e93-87a1-d38d0994162c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'article', 'is', 'about', 'the']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "id": "1pZ7Gz1lunr7",
        "outputId": "a5ccfaf7-9b40-42f3-baf7-5d563e7974ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization for every words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer() # WordNet Lemmatizer adalah untuk konversi kata-kata menjadi kata dasar seperti cars -> car dan tidak berlaku jika am -> be\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation) # Membuat dictionary punctuation menjadi None\n",
        "def LemTokens(tokens):\n",
        "  return [wnl.lemmatize(token) for token in tokens]\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "P6BgJ0j_SZXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Greeting for Bot\n",
        "GREET_INPUTS = (\"hello\", \"hi\", \"greetings\", \"what's up\", \"hey\")\n",
        "GREET_RESPONSES = [\"hi\", \"hey\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in GREET_INPUTS:\n",
        "      return random.choice(GREET_RESPONSES)"
      ],
      "metadata": {
        "id": "aYW9dhncUh5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Important library for BOT Response\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Membuat vektor beserta nilai tf-idf\n",
        "from sklearn.metrics.pairwise import cosine_similarity # Mencari jarak atau nilai paling sama pada hasil tf-idf"
      ],
      "metadata": {
        "id": "xRvUbHtnausI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BOT Response\n",
        "def response(user_response):\n",
        "  bot_response = \"\"\n",
        "  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize) # Memakai tokenizer yang dibuat sediri\n",
        "  tfidf_results = TfidfVec.fit_transform(sentence_tokens) # Mnedapatkan vektor atau nilai tf-idf\n",
        "  word_similar = cosine_similarity(tfidf_results[-1], tfidf_results) # Membandingkan kemiripan user_response dan tfidf_results\n",
        "  idx = word_similar.argsort()[0][-2] # Mencari index kedua terakhir karena yang paling mirip dengan user_response (input sentence) [0]: mengambil array pertama (word_similar array memang hanya 1) [-2]: Mengambil index kedua terakhir karena yang terakhir adalah user_response\n",
        "  word_similar_to_flat = word_similar.flatten()\n",
        "  word_similar_to_flat.sort()\n",
        "  req_tfidf = word_similar_to_flat[-2] # Mengambil nilai tf-idf pada index terakhir (karena yang paling besar sehingga bisa dikatakan yang paling mirip)\n",
        "  if (req_tfidf == 0):\n",
        "    bot_response = bot_response + \"I am sorry! I don't understand you\"\n",
        "    return bot_response\n",
        "  else:\n",
        "    bot_response = bot_response + sentence_tokens[idx] # sentence_tokens[idx]: mengambil sentence asli pada index yang sudah dihitung tadi yang paling mirip\n",
        "    return bot_response"
      ],
      "metadata": {
        "id": "b3JZeyDsbQFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BOT Conversation\n",
        "talk = True\n",
        "print(\"BOT: Hello, can i help you?. Please type 'Bye' if you exit üôè\")\n",
        "while (talk == True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if (user_response !=  'bye'):\n",
        "    if (user_response == 'thanks' or user_response == 'thank you'):\n",
        "      talk = False\n",
        "      print(\"BOT: You're Welcome\")\n",
        "    else:\n",
        "      if (greet(user_response) != None):\n",
        "        print(\"BOT: \" + greet(user_response))\n",
        "      else:\n",
        "        sentence_tokens.append(user_response)\n",
        "        word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
        "        # final_words = list(set(word_tokens))\n",
        "        print(\"BOT: \", end=\"\")\n",
        "        print(response(user_response))\n",
        "        sentence_tokens.remove(user_response)\n",
        "  else:\n",
        "    talk = False\n",
        "    print(\"BOT: Good bye! Have a nice day\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuX_m9gqg7s_",
        "outputId": "8ef613bd-423f-4182-df33-8b89eebcde15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOT: Hello, can i help you?. Please type 'Bye' if you exit üôè\n",
            "Bye\n",
            "BOT: Good bye! Have a nice day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kesimpulan"
      ],
      "metadata": {
        "id": "Td9F5G69xmgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proses utama dalam membuat sistem adalah:\n",
        "1. Query Document\n",
        "2. Pre-Processing Text\n",
        "3. Processing Text\n",
        "4. Bot System"
      ],
      "metadata": {
        "id": "5Zr2VC-gydfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Query Document\n",
        "- Dokumen diambil dari GitHub\n",
        "- Dokumen berisi artikel atau informasi mengenai topik tertentu.\n",
        "- Dokumen digunakan oleh bot untuk mencari jawaban dari pertanyaan user\n",
        "- Tanpa dokumen bot tidak bisa menjawab pertanyaan dengan baik\n",
        "- Semakin banyak dokumen maka bot semakin pintar\n",
        "\n",
        "### 2. Pre-Processing Text\n",
        "- Lowercase\n",
        "- Tokenize text perkalimat dan perkata\n",
        "- Membuang tanda baca\n",
        "- Vectorization\n",
        "\n",
        "### 3. Processing Text\n",
        "- Menghitung nilai TF-IDF pada dokumen, TF-IDF dihitung berdasarkan kalimat pada dokumen tersebut. TF-IDF memberikan nilai pada setiap kalimat.\n",
        "\n",
        "- Cosine Similar, dilakukan untuk mencari nilai terdekat atau paling mirip antara ‚Äúuser input‚Äù dan ‚Äúkalimat pada dokumen‚Äù\n",
        "\n",
        "### 4. Bot Systems\n",
        "\n",
        "- BOT Greeting (Sapaan dari BOT)\n",
        "- BOT Response, mengatur semua proses yang dilakukan secara berurutan seperti user memasukan input, kalimat user di pre-process dan bot memberikan respon dari hasil cosine similar dari kalimat pertanyaan input user\n"
      ],
      "metadata": {
        "id": "WdgLTUSMzG-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perubahan Modifikasi"
      ],
      "metadata": {
        "id": "GR_RmCIUzyyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Modifkasi Data\n",
        "Data yang sebelumnya artikel mengenai chatbot, maka saya ganti menjadi artikel cancer yang say ambil dari wikipedia (https://en.wikipedia.org/wiki/Cancer), artikel ini langsung saya copy mulai dari baris awal sampai akhir tanpa memperhatikan apapun, karena didalam sistem ini sudah ada preprocess\n",
        "\n",
        "### 2. Modifikasi Variabel\n",
        "Ada beberapa variabel yang membingungkan seperti \"sent_tokens\" saya rubah menjadi \"sentence_tokens\", hal ini dikarenakan arti dari \"sent\" adalah kirim, saya mulanya mencari tahu, kenapa harus \"sent\" ternyata yang dimaksud adalah \"sentences\","
      ],
      "metadata": {
        "id": "Oja6WGih0Afu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adapun Kekurangan dalam sistem ini adalah:\n",
        "- Tidak ada remove stopwords pada saat preprocessing\n",
        "- Proses stemming tidak dilakukan (Hanya lemmatization saja)\n",
        "- TF-IDF masih menggunakan library\n",
        "- Pre-processing penghilangan kata \"[60]\" atau \"[48]\" atau yang lain masih ada (bisa dilihat pada output Chatbots)"
      ],
      "metadata": {
        "id": "Yxk5uZ2CHf3j"
      }
    }
  ]
}